{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_inf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM8BzMYIyUO8RPuwCtkhg/V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tezike/Show-attend-and-tell/blob/master/attention_inf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XzUrPpZCbR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNMpGi7wubtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip -q install fastai2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VFh4FDBFKrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import gc\n",
        "import dill as pickler\n",
        "import joblib as picklizer\n",
        "import fastai\n",
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from fastai2.vision.core import *\n",
        "from torchvision import transforms\n",
        "pickler._dill._reverse_typemap['ClassType'] = type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drG5V2qAH11N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('models'): os.mkdir('models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIsgLAsrFLkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp '/content/drive/My Drive/Image_Captioning/models/vocab_coco.pkl' '/content/models/vocab_coco.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjX-m9HSFiJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp '/content/drive/My Drive/Image_Captioning/models/models/mygoodmodel_coco.pth' '/content/models/mygoodmodel_coco.pth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-eIuTjyv-PP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp '/content/drive/My Drive/Image_Captioning/models/loaded_learn_cpu.pkl' '/content/models/loaded_learn_cpu.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EotKy3sjLyis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp '/content/drive/My Drive/Image_Captioning/models/learn_cpu.pkl' '/content/models/learn_cpu.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGrX0bRwrPk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone --quiet 'https://github.com/tezike/download_google_drive.git'\n",
        "os.chdir('download_google_drive')\n",
        "# !python download_gdrive.py '1-QlLAWm3L1-Jyjj0R48tvphWdNP0O006' '../models/learn_cpu.pkl'\n",
        "!python download_gdrive.py '1-_BHQhAuVl_PInF1GWO3x54sQv30XdMW' '../models/loaded_learn_cpu.pkl'\n",
        "# !python download_gdrive.py '1-6bUIv2R12OmoE6X3a2MPLco0Uj7Mw4P' '../models/mygoodmodel_coco.pth'\n",
        "!python download_gdrive.py '1-kTk4u9r7M9OY6f-qBQieakQTe17B_8B' '../models/vocab_coco.pkl'\n",
        "shutil.rmtree('../download_google_drive')\n",
        "os.chdir('..')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKjXge1XM1SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K8fTGLiCa-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUKY2NZbu0Ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNHg9WlxCnhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = pickle.load(open('models/vocab_coco.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-LdKZ3LGU2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_layers, attn_size = 1, 500\n",
        "sz = 224"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R3pl1BWGa3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# last layers of resnet\n",
        "def fc_layer(in_, out, p=0.1):\n",
        "    return nn.Sequential(\n",
        "    Flatten(),\n",
        "    nn.Linear(in_, out),\n",
        "    nn.Dropout(p),\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edGcY9TKCncN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, device, dec_hidden_state_size, dec_layers, filter_width, num_filters):\n",
        "        super().__init__()\n",
        "        # Visual Encoder\n",
        "        self.device = device\n",
        "        self.base_network = nn.Sequential(*list(models.resnet101(pretrained=True).children())[:-2])\n",
        "        self.freeze_base_network()\n",
        "        self.concatPool = AdaptiveConcatPool2d(sz=1)\n",
        "        self.adaptivePool = nn.AdaptiveAvgPool2d((filter_width, filter_width))\n",
        "        self.filter_width = filter_width\n",
        "        \n",
        "        self.output_layers = nn.ModuleList([\n",
        "            fc_layer(2*num_filters, dec_hidden_state_size) for _ in range(dec_layers)\n",
        "        ])\n",
        "          \n",
        "    def forward(self, inp):\n",
        "        #pdb.set_trace()\n",
        "        enc_output = self.base_network(inp)\n",
        "        annotation_vecs = self.adaptivePool(enc_output).view(enc_output.size(0), enc_output.size(1), -1)\n",
        "        enc_output = self.concatPool(enc_output)\n",
        "        \n",
        "        dec_init_hidden_states = [MLP_layer(enc_output) for MLP_layer in self.output_layers]\n",
        "        \n",
        "        return torch.stack(dec_init_hidden_states, dim = 0), annotation_vecs.transpose(1, 2)\n",
        "    \n",
        "    def freeze_base_network(self):\n",
        "        for layer in self.base_network:\n",
        "            requires_grad(layer, False)\n",
        "            \n",
        "    def fine_tune(self, from_block=-1):\n",
        "        for layer in self.base_network[from_block:]:\n",
        "            requires_grad(layer, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSWrs59XCwNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VisualAttention(nn.Module):\n",
        "    def __init__(self, num_filters, dec_dim, att_dim):\n",
        "        super().__init__()\n",
        "        self.attend_annot_vec = nn.Linear(num_filters, att_dim)\n",
        "        self.attend_dec_hidden= nn.Linear(dec_dim, att_dim)\n",
        "        self.f_att = nn.Linear(att_dim, 1)  # Equation (4) in Xu et al. (2015)\n",
        "        \n",
        "    def forward(self, annotation_vecs, dec_hid_state):\n",
        "        #pdb.set_trace()\n",
        "        attended_annotation_vecs = self.attend_annot_vec(annotation_vecs)\n",
        "        attended_dec_hid_state   = self.attend_dec_hidden(dec_hid_state)\n",
        "        e = self.f_att(F.relu(attended_annotation_vecs + attended_dec_hid_state.unsqueeze(1))).squeeze(2)  # Eq. 4\n",
        "        alphas = F.softmax(e, dim=1)  # Equation (5) in Xu et al. (2015)\n",
        "        context_vec = (annotation_vecs * alphas.unsqueeze(2)).sum(1)  # Equations (13)\n",
        "        \n",
        "        return context_vec, alphas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6SFCXCtC0Tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, device, filter_width, num_filters, vocab_size, emb_sz, out_seqlen, n_layers=3, prob_teach_forcing=1, p_drop=0.3):\n",
        "        super().__init__()\n",
        "        self.n_layers, self.out_seqlen = n_layers, out_seqlen\n",
        "        self.filter_width = filter_width\n",
        "        self.num_filters = num_filters\n",
        "        self.device = device  \n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = Encoder(device, emb_sz, n_layers, filter_width, num_filters)\n",
        "        \n",
        "        # Attention\n",
        "        self.att = VisualAttention(num_filters, emb_sz, 500)\n",
        "        \n",
        "        # Decoder\n",
        "        self.emb = nn.Embedding(vocab_size, emb_sz) #create_emb(wordvecs, itos, emb_sz)\n",
        "        self.rnn_dec = nn.GRU(num_filters + emb_sz, emb_sz, num_layers=n_layers, dropout=0 if n_layers == 1 else p_drop)  # square to enable weight tying\n",
        "        self.out_drop = nn.Dropout(p_drop)\n",
        "        self.out = nn.Linear(emb_sz, vocab_size)\n",
        "        self.out.weight.data = self.emb.weight.data\n",
        "        self.f_b = nn.Linear(emb_sz, num_filters)  # Section 4.2.1 in Xu et al. (2015)\n",
        "        \n",
        "        self.prob_teach_forcing = prob_teach_forcing\n",
        "        self.initializer()\n",
        "        \n",
        "    def initializer(self):\n",
        "        self.emb.weight.data.uniform_(-0.1, 0.1)\n",
        "        \n",
        "    def forward(self, x, y=None):\n",
        "        #pdb.set_trace()\n",
        "        h, annotation_vecs = self.encode(x)\n",
        "\n",
        "        dec_inp = torch.zeros(h.size(1), requires_grad=False).long()\n",
        "        dec_inp = dec_inp.to(self.device)\n",
        "        res = []\n",
        "        alphas = []\n",
        "        \n",
        "        for i in range(self.out_seqlen):\n",
        "            #pdb.set_trace()\n",
        "            dec_output, h, alpha = self.decode_step(dec_inp, h, annotation_vecs)\n",
        "            res.append(dec_output)\n",
        "            alphas.append(alpha)\n",
        "            \n",
        "            if (dec_inp == 1).all() or (y is not None and i >= len(y)):\n",
        "                break            \n",
        "            # teacher forcing\n",
        "            elif y is not None and (self.prob_teach_forcing > 0) and (random.random() < self.prob_teach_forcing):\n",
        "                dec_inp = y[i].to(self.device)\n",
        "            else:\n",
        "                dec_inp = dec_output.data.max(1)[1]  # [1] to get argmax\n",
        "        \n",
        "        return torch.stack(res), torch.stack(alphas)\n",
        "    \n",
        "    def encode(self, x):\n",
        "        return self.encoder(x.to(self.device))\n",
        "    \n",
        "    def decode_step(self, dec_inp, h, annotation_vecs):\n",
        "        #pdb.set_trace()\n",
        "        context_vec, alpha = self.att(annotation_vecs, h[-1])\n",
        "        beta = torch.sigmoid(self.f_b(h[-1]))\n",
        "        context_vec = beta * context_vec  # Section 4.2.1 in Xu et al. (2015)\n",
        "        \n",
        "        emb_inp = self.emb(dec_inp).unsqueeze(0)  # adds unit axis at beginning so that rnn 'loops' once\n",
        "\n",
        "        output, h = self.rnn_dec(torch.cat([emb_inp, context_vec.unsqueeze(0)], dim=2), h)\n",
        "        output = self.out(self.out_drop(output[0]))\n",
        "\n",
        "        # return F.log_softmax(output, dim=1), h, alpha\n",
        "        return F.log_softmax(output, dim=1), h, alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCyXa-3TC9lC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CWfULVJ_gDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = Decoder(device, 7, 2048, len(vocab.itos), 1000, 50, n_layers, p_drop=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a5_DYoPC-RL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s269W7TDzSRe",
        "colab": {}
      },
      "source": [
        "x_ = gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ig0HRQnLFhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learn = pickler.load(open('models/learn_cpu.pkl', 'rb')) #use this"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrF-0K6IqJBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = picklizer.load(open('models/loaded_learn_cpu.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdB1V8e9x2x9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from fastai.callbacks import SaveModelCallback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMbfmmstqM7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learn.callback = SaveModelCallback(learn)\n",
        "# learn.loss_func = None\n",
        "# learn.metrics = None\n",
        "# learn.purge()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40YuPAs0NtDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learn.model_dir = Path('models/')\n",
        "# learn.path = Path('.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xpGuLA2nYqll",
        "colab": {}
      },
      "source": [
        "# learn = learn.load('mygoodmodel_coco') #and this to create new_loaded joblib\n",
        "# learn.data = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTzNTSZk01_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# picklizer.dump(learn.model, open('/content/drive/My Drive/Image_Captioning/models/loaded_learn_cpu.pkl', 'wb'), compress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkBX0WEvLP5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXQ7kMmKoIAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decode_eval = learn.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C61zzZItDnSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HypothesisNode():\n",
        "    \"\"\" Hypothesis Node class for performing Beam Search \"\"\"\n",
        "    def __init__(self, sequence, log_prob, hidden_state, alphas):\n",
        "        \"\"\"HypothesisNode constructur\n",
        "        \n",
        "        Args:\n",
        "          sequence: A sequence of tokens\n",
        "          log_prob: The log of the probability of this sequence\n",
        "          hidden_state: The hidden state of the Decoder RNN after decoding the last token in the sequence\n",
        "        \"\"\"\n",
        "        self._seq = sequence\n",
        "        self._alphas = alphas\n",
        "        self._log_prob = log_prob\n",
        "        self._h = hidden_state\n",
        "    \n",
        "    @property\n",
        "    def last_tok(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          The last token in the sequence\n",
        "        \"\"\"\n",
        "        return self._seq[-1]\n",
        "        \n",
        "    def update(self, tok, log_prob, new_h, new_alpha):\n",
        "        \"\"\"\n",
        "        Updates the sequence with a new token and returns a new Hypothesis Node\n",
        "        Args:\n",
        "          tok: The new token that is appended to the sequence\n",
        "          log_prob: The log of the probability ot this token\n",
        "          new_h: The new hidden state of the Decoder RNN after this token\n",
        "        \n",
        "        Returns:\n",
        "          An Hypothesis Node with the updated sequence, log probability and hidden state\n",
        "        \"\"\"\n",
        "        return HypothesisNode(self._seq + [tok], self._log_prob + log_prob, new_h, self._alphas + new_alpha)\n",
        "    \n",
        "    def __str__(self):\n",
        "        return ('Hyp(log_p = %4f,\\t seq = %s)' % (self._log_prob, vocab.textify([t.item()for t in self._seq])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3gtO4OBDswO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BeamSearch():\n",
        "    \"\"\" Performs BeamSearch for seq2seq decoding or Image captioning \"\"\"\n",
        "    def __init__(self, enc_model, dec_model, beam_width=5, num_results=1, max_len=30, device=torch.device('cuda:0')):\n",
        "        \"\"\"BeamSearch object constructor\n",
        "        Args:\n",
        "          enc_model: A seq2seq encoder or cnn for image captioning\n",
        "          dec_model: A RNN decoder model\n",
        "          beam_width: int, the number of hypotheses to remember in each iteration\n",
        "          max_len: int, the longest possible sequence\n",
        "        \"\"\"\n",
        "        self._device = device\n",
        "        self._enc_model = enc_model\n",
        "        self._dec_model = dec_model\n",
        "        self._beam_width = beam_width\n",
        "        self._num_results = num_results\n",
        "        self._max_len = max_len\n",
        "        self._start_tok = 0\n",
        "        self._end_tok   = 1\n",
        "        self._annotation_vecs = None\n",
        "        \n",
        "    def __call__(self, img, verbose=False):\n",
        "        \"\"\"Performs the Beam search\n",
        "        Args:\n",
        "          img: the image to be annotated, torch tensor with 3 color channels\n",
        "          verbose: bool, allows printing the intermediate hypotheses for better understanding\n",
        "        \n",
        "        Returns:\n",
        "          The 'beam_width' most probable sentences\n",
        "        \"\"\"\n",
        "        img = img.unsqueeze(0)\n",
        "        h, annotation_vecs = self._enc_model(img)\n",
        "        self._annotation_vecs = annotation_vecs\n",
        "        \n",
        "        hyps = [HypothesisNode([torch.zeros(1, requires_grad=False).long().to(self._device)], 0, h, [])]\n",
        "        results = []\n",
        "        \n",
        "        step = 0\n",
        "        width = self._beam_width\n",
        "        while width > 0 and step < self._max_len:\n",
        "            if verbose: print(\"\\n Step: \",step)\n",
        "            new_hyps = []\n",
        "            for h in hyps:\n",
        "                new_hyps.extend(self.get_next_hypotheses(h, width))\n",
        "            \n",
        "            new_hyps = sorted(new_hyps, key= lambda x: x._log_prob, reverse=True)\n",
        "            if verbose: self.print_hypotheses(new_hyps, \"Before narrowing:\")\n",
        "                \n",
        "            hyps = []\n",
        "            for h in new_hyps[:width]:\n",
        "                if h.last_tok == self._end_tok:\n",
        "                    results.append(h)\n",
        "                    width = width - 1\n",
        "                else:\n",
        "                    hyps.append(h)\n",
        "            \n",
        "            if verbose: \n",
        "                self.print_hypotheses(hyps, \"After narrowing:\")\n",
        "                self.print_hypotheses(results, \"Results:\")\n",
        "                \n",
        "            step += 1\n",
        "         \n",
        "        results.extend(hyps[:width])\n",
        "        results = sorted(results, key=lambda x: x._log_prob/len(x._seq), reverse=True)\n",
        "        \n",
        "        if verbose: self.print_hypotheses(results, \"Final:\")\n",
        "        \n",
        "        if self._num_results == 1:\n",
        "            return ([t.item() for t in results[0]._seq[1:-1]], torch.stack(results[0]._alphas))\n",
        "        else:\n",
        "            return [([t.item() for t in r._seq[1:-1]], torch.stack(r._alphas)) for r in results[:self._num_results]]\n",
        "        \n",
        "    def get_next_hypotheses(self, hyp, k):\n",
        "        \"\"\"Calculates the next 'beam_width' hypotheses given a Hypothesis Node\n",
        "        Args:\n",
        "          hyp: an Hypothesis Node containing a sequence, a log probability and a Decoder RNN hidden state\n",
        "          k: the number of hypotheses to calculate\n",
        "        Returns:\n",
        "          A list with the 'beam_width' most probable sequences/Hypothesis Nodes\n",
        "        \"\"\"\n",
        "\n",
        "        dec_outp, h, alphas = self._dec_model(hyp.last_tok, hyp._h, self._annotation_vecs)\n",
        "\n",
        "        top_k_log_probs, top_k_toks = dec_outp.topk(k, dim=1)\n",
        "        return [hyp.update(top_k_toks[0][i].unsqueeze(0), top_k_log_probs[0][i], h, list(alphas)) for i in range(k)]\n",
        "    \n",
        "    def print_hypotheses(self, hyps, description):\n",
        "        print(description)\n",
        "        for h in hyps:\n",
        "            print(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0ayjUa1D2b8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj6tC2sv6S3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from scipy.misc import imresize\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from matplotlib.patheffects import Stroke, Normal\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# the functions fig2data and fig2img are taken from \n",
        "# http://www.icare.univ-lille1.fr/tutorials/convert_a_matplotlib_figure\n",
        "# Deprecation errors have been fixed\n",
        "\n",
        "def fig2data ( fig ):\n",
        "    \"\"\"\n",
        "    @brief Convert a Matplotlib figure to a 4D numpy array with RGBA channels and return it\n",
        "    @param fig a matplotlib figure\n",
        "    @return a numpy 3D array of RGBA values\n",
        "    \"\"\"\n",
        "    # draw the renderer\n",
        "    fig.canvas.draw ( )\n",
        " \n",
        "    # Get the RGBA buffer from the figure\n",
        "    w,h = fig.canvas.get_width_height()\n",
        "    buf = np.fromstring( fig.canvas.tostring_argb(), dtype=np.uint8 )\n",
        "    buf.shape = ( w, h,4 )\n",
        " \n",
        "    # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode\n",
        "    buf = np.roll ( buf, 3, axis = 2 )\n",
        "    return buf\n",
        "\n",
        "def fig2img ( fig ):\n",
        "    \"\"\"\n",
        "    @brief Convert a Matplotlib figure to a PIL Image in RGBA format and return it\n",
        "    @param fig a matplotlib figure\n",
        "    @return a Python Imaging Library ( PIL ) image\n",
        "    \"\"\"\n",
        "    # put the figure pixmap into a numpy array\n",
        "    buf = fig2data ( fig )\n",
        "    w, h, d = buf.shape\n",
        "    return Image.frombytes( \"RGBA\", ( w ,h ), buf.tostring( ) )\n",
        "    \n",
        "def draw_text(ax, xy, txt, sz=14):\n",
        "    text = ax.text(*xy, txt, verticalalignment='top', color='white', fontsize=sz, weight='bold')\n",
        "    draw_outline(text, 1)\n",
        "\n",
        "def draw_outline(matplt_plot_obj, lw):\n",
        "    matplt_plot_obj.set_path_effects([Stroke(linewidth=lw, foreground='black'), Normal()])\n",
        "\n",
        "def show_img(im, figsize=None, ax=None, alpha=1, cmap=None):\n",
        "    if not ax:\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.imshow(im, alpha=alpha, cmap=cmap)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    return ax\n",
        "\n",
        "def visualize_attention(im, pred, alphas, denorm, vocab, att_size=7, thresh=0., sz=224, return_fig_as_PIL_image=False):\n",
        "    cap_len = len(pred)\n",
        "    alphas = alphas.view(-1,1,  att_size, att_size).cpu().data.numpy()\n",
        "    alphas = np.maximum(thresh, alphas)\n",
        "    alphas -= alphas.min()\n",
        "    alphas /= alphas.max()\n",
        " \n",
        "    figure, axes = plt.subplots(cap_len//5 + 1,5, figsize=(12,8))\n",
        " \n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i <= cap_len:\n",
        "            ax = show_img(denorm(im), ax=ax)\n",
        "            if i > 0:\n",
        "                mask = np.array(PIL.Image.fromarray(alphas[i - 1,0]).resize((sz,sz)))\n",
        "                blurred_mask = gaussian_filter(mask, sigma=8)\n",
        "                show_img(blurred_mask, ax=ax, alpha=0.5, cmap='afmhot')\n",
        "                draw_text(ax, (0,0), vocab.itos[pred[i - 1]])\n",
        "        else:\n",
        "            ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if return_fig_as_PIL_image:\n",
        "        return fig2img(figure)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biofXRQg1nGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beam_width = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "298W3na74Pb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qaXM8H3X8gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beam_search = BeamSearch(learn.encode, learn.decode_step, beam_width, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPWKuBfXPuIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYGCeM_BY8s6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# url = 'https://st2.depositphotos.com/1761942/9533/i/950/depositphotos_95337166-stock-photo-two-kid-boys-playing-on.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-91GNBdUZTZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mkdir images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCmaC_VGZP_0",
        "colab_type": "code",
        "outputId": "2d19bb11-a290-4a50-dcc0-c0c81e933a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "# download_url(url, 'images/rand2.jpg', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAfryLkpJaBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inv_norm = transforms.Normalize(\n",
        "    mean =  [-0.5238/0.3159, -0.5003/0.3091, -0.4718/0.3216],\n",
        "    std = [1/0.3159, 1/0.3091, 1/0.3216]\n",
        ")\n",
        "\n",
        "denorm = transforms.Compose([\n",
        "                            inv_norm,\n",
        "                            # make the image PIL readable\n",
        "                            transforms.functional.to_pil_image\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "231smyO55BZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# im = image2tensor(PILImage.create('images/rand2.jpg').to_thumb(224))/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTRtYFc6HOJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.imshow(im.permute(1,2,0)), im.permute(1,2,0).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo2qZAU3EDOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# results = beam_search(im);\n",
        "# x = vocab.textify(results[0])\n",
        "# print(x)\n",
        "# visualize_attention(im, results[0], results[1], denorm, vocab, att_size=7, sz=sz, thresh=0.02)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6yDE-SsJthy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6NvMYHXOS9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ipywidgets\n",
        "from ipywidgets import widgets, VBox\n",
        "# from types import SimpleNamespace\n",
        "import warnings\n",
        "warnings. filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17U3NjvcAF7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defaults.use_cuda = True\n",
        "text = widgets.Text()\n",
        "output = widgets.Output()\n",
        "output_attn = widgets.Output()\n",
        "output_lbl = widgets.Output()\n",
        "label = widgets.Label()\n",
        "caption_lbl = widgets.Label()\n",
        "attend = widgets.Button(description='Caption eet!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWqecePnjGvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyl9doT0CaTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_click(change):\n",
        "    # if len(upload.data)!=0:\n",
        "    if len(upload_btn.data)!=0:\n",
        "        img = PILImage.create(upload_btn.data[-1])\n",
        "        output.clear_output()\n",
        "        with output:\n",
        "            display(img.to_thumb(224))\n",
        "            \n",
        "        gc.collect()\n",
        "        results = beam_search(image2tensor(img.to_thumb(700))/255.);\n",
        "        x = vocab.textify(results[0])\n",
        "        with output_lbl:\n",
        "            print(x)\n",
        "        pred_caption = vocab.textify(results[0])\n",
        "        caption_lbl.value = pred_caption\n",
        "        output_attn.clear_output()\n",
        "        with output_attn:\n",
        "            visualize_attention(image2tensor(img.to_thumb(224))/255., results[0], results[1], denorm, vocab, att_size=7, sz=sz, thresh=0.02)\n",
        "    else: print('Please upload an image')\n",
        "\n",
        "attend.on_click(on_click)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO0PDjKciFlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# upload = SimpleNamespace(data=[open_image('images/rand2.jpg')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybeDssyJCOX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upload_btn = widgets.FileUpload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PvSUqEdNMQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_ = gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYJ8bhsvEBQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(VBox([widgets.Label('Attend to something'), \n",
        "      upload_btn, attend, output, output_lbl, caption_lbl]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}