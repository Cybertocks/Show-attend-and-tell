{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_inf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPU1fTE3NjNkGz0ec9xvr6F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de1ac3a6a7ec4ac5b1bbb6018b301915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7d2c21f8bde346aa8ba27be35d2ad06c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e95113badecd419e8269d082d1d8348b",
              "IPY_MODEL_2193d80a1e6f4d939413efe592e0b22d"
            ]
          }
        },
        "7d2c21f8bde346aa8ba27be35d2ad06c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e95113badecd419e8269d082d1d8348b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_39b0cb32e3db4a568e6d574d790ca7bf",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 178728960,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178728960,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83499b3ad71440818edc8b71078b310d"
          }
        },
        "2193d80a1e6f4d939413efe592e0b22d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2b16c0bf210347bb82e1bda55854d7f2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170M/170M [00:02&lt;00:00, 67.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5dc0c90163864a18b1b57043778892ab"
          }
        },
        "39b0cb32e3db4a568e6d574d790ca7bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83499b3ad71440818edc8b71078b310d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b16c0bf210347bb82e1bda55854d7f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5dc0c90163864a18b1b57043778892ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tezike/Show-attend-and-tell/blob/master/attention_inf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XzUrPpZCbR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNMpGi7wubtF",
        "colab_type": "code",
        "outputId": "9c9cf5d6-01d0-404f-aad9-ea30be06d25d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !pip -q install fastai2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▉                              | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 2.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VFh4FDBFKrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import gc\n",
        "import dill as pickler\n",
        "import joblib as picklizer\n",
        "import fastai\n",
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from fastai2.vision.core import *\n",
        "from torchvision import transforms\n",
        "pickler._dill._reverse_typemap['ClassType'] = type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drG5V2qAH11N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('models'): os.mkdir('models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIsgLAsrFLkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp '/content/drive/My Drive/Image_Captioning/models/vocab_coco.pkl' '/content/models/vocab_coco.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjX-m9HSFiJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp '/content/drive/My Drive/Image_Captioning/models/models/mygoodmodel_coco.pth' '/content/models/mygoodmodel_coco.pth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-eIuTjyv-PP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp '/content/drive/My Drive/Image_Captioning/models/loaded_learn_cpu.pkl' '/content/models/loaded_learn_cpu.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EotKy3sjLyis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp '/content/drive/My Drive/Image_Captioning/models/learn_cpu.pkl' '/content/models/learn_cpu.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGrX0bRwrPk4",
        "colab_type": "code",
        "outputId": "6c00adb5-8cb9-4d22-f573-517058a0df67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!git clone 'https://github.com/chentinghao/download_google_drive.git'\n",
        "os.chdir('download_google_drive')\n",
        "# !python download_gdrive.py '1-QlLAWm3L1-Jyjj0R48tvphWdNP0O006' '../models/learn_cpu.pkl'\n",
        "!python download_gdrive.py '1-_BHQhAuVl_PInF1GWO3x54sQv30XdMW' '../models/loaded_learn_cpu.pkl'\n",
        "# !python download_gdrive.py '1-6bUIv2R12OmoE6X3a2MPLco0Uj7Mw4P' '../models/mygoodmodel_coco.pth'\n",
        "!python download_gdrive.py '1-kTk4u9r7M9OY6f-qBQieakQTe17B_8B' '../models/vocab_coco.pkl'\n",
        "shutil.rmtree('../download_google_drive')\n",
        "os.chdir('..')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'download_google_drive'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Total 16 (delta 0), reused 0 (delta 0), pack-reused 16\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n",
            "758MB [00:07, 101MB/s]\n",
            "758MB [00:08, 89.2MB/s]\n",
            "288kB [00:00, 134MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKjXge1XM1SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K8fTGLiCa-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUKY2NZbu0Ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNHg9WlxCnhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = pickle.load(open('models/vocab_coco.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-LdKZ3LGU2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_layers, attn_size = 1, 500\n",
        "sz = 224"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R3pl1BWGa3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# last layers of resnet\n",
        "def fc_layer(in_, out, p=0.1):\n",
        "    return nn.Sequential(\n",
        "    Flatten(),\n",
        "    nn.Linear(in_, out),\n",
        "    nn.Dropout(p),\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edGcY9TKCncN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, device, dec_hidden_state_size, dec_layers, filter_width, num_filters):\n",
        "        super().__init__()\n",
        "        # Visual Encoder\n",
        "        self.device = device\n",
        "        self.base_network = nn.Sequential(*list(models.resnet101(pretrained=True).children())[:-2])\n",
        "        self.freeze_base_network()\n",
        "        self.concatPool = AdaptiveConcatPool2d(sz=1)\n",
        "        self.adaptivePool = nn.AdaptiveAvgPool2d((filter_width, filter_width))\n",
        "        self.filter_width = filter_width\n",
        "        \n",
        "        self.output_layers = nn.ModuleList([\n",
        "            fc_layer(2*num_filters, dec_hidden_state_size) for _ in range(dec_layers)\n",
        "        ])\n",
        "          \n",
        "    def forward(self, inp):\n",
        "        #pdb.set_trace()\n",
        "        enc_output = self.base_network(inp)\n",
        "        annotation_vecs = self.adaptivePool(enc_output).view(enc_output.size(0), enc_output.size(1), -1)\n",
        "        enc_output = self.concatPool(enc_output)\n",
        "        \n",
        "        dec_init_hidden_states = [MLP_layer(enc_output) for MLP_layer in self.output_layers]\n",
        "        \n",
        "        return torch.stack(dec_init_hidden_states, dim = 0), annotation_vecs.transpose(1, 2)\n",
        "    \n",
        "    def freeze_base_network(self):\n",
        "        for layer in self.base_network:\n",
        "            requires_grad(layer, False)\n",
        "            \n",
        "    def fine_tune(self, from_block=-1):\n",
        "        for layer in self.base_network[from_block:]:\n",
        "            requires_grad(layer, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSWrs59XCwNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VisualAttention(nn.Module):\n",
        "    def __init__(self, num_filters, dec_dim, att_dim):\n",
        "        super().__init__()\n",
        "        self.attend_annot_vec = nn.Linear(num_filters, att_dim)\n",
        "        self.attend_dec_hidden= nn.Linear(dec_dim, att_dim)\n",
        "        self.f_att = nn.Linear(att_dim, 1)  # Equation (4) in Xu et al. (2015)\n",
        "        \n",
        "    def forward(self, annotation_vecs, dec_hid_state):\n",
        "        #pdb.set_trace()\n",
        "        attended_annotation_vecs = self.attend_annot_vec(annotation_vecs)\n",
        "        attended_dec_hid_state   = self.attend_dec_hidden(dec_hid_state)\n",
        "        e = self.f_att(F.relu(attended_annotation_vecs + attended_dec_hid_state.unsqueeze(1))).squeeze(2)  # Eq. 4\n",
        "        alphas = F.softmax(e, dim=1)  # Equation (5) in Xu et al. (2015)\n",
        "        context_vec = (annotation_vecs * alphas.unsqueeze(2)).sum(1)  # Equations (13)\n",
        "        \n",
        "        return context_vec, alphas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6SFCXCtC0Tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, device, filter_width, num_filters, vocab_size, emb_sz, out_seqlen, n_layers=3, prob_teach_forcing=1, p_drop=0.3):\n",
        "        super().__init__()\n",
        "        self.n_layers, self.out_seqlen = n_layers, out_seqlen\n",
        "        self.filter_width = filter_width\n",
        "        self.num_filters = num_filters\n",
        "        self.device = device  \n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = Encoder(device, emb_sz, n_layers, filter_width, num_filters)\n",
        "        \n",
        "        # Attention\n",
        "        self.att = VisualAttention(num_filters, emb_sz, 500)\n",
        "        \n",
        "        # Decoder\n",
        "        self.emb = nn.Embedding(vocab_size, emb_sz) #create_emb(wordvecs, itos, emb_sz)\n",
        "        self.rnn_dec = nn.GRU(num_filters + emb_sz, emb_sz, num_layers=n_layers, dropout=0 if n_layers == 1 else p_drop)  # square to enable weight tying\n",
        "        self.out_drop = nn.Dropout(p_drop)\n",
        "        self.out = nn.Linear(emb_sz, vocab_size)\n",
        "        self.out.weight.data = self.emb.weight.data\n",
        "        self.f_b = nn.Linear(emb_sz, num_filters)  # Section 4.2.1 in Xu et al. (2015)\n",
        "        \n",
        "        self.prob_teach_forcing = prob_teach_forcing\n",
        "        self.initializer()\n",
        "        \n",
        "    def initializer(self):\n",
        "        self.emb.weight.data.uniform_(-0.1, 0.1)\n",
        "        \n",
        "    def forward(self, x, y=None):\n",
        "        #pdb.set_trace()\n",
        "        h, annotation_vecs = self.encode(x)\n",
        "\n",
        "        dec_inp = torch.zeros(h.size(1), requires_grad=False).long()\n",
        "        dec_inp = dec_inp.to(self.device)\n",
        "        res = []\n",
        "        alphas = []\n",
        "        \n",
        "        for i in range(self.out_seqlen):\n",
        "            #pdb.set_trace()\n",
        "            dec_output, h, alpha = self.decode_step(dec_inp, h, annotation_vecs)\n",
        "            res.append(dec_output)\n",
        "            alphas.append(alpha)\n",
        "            \n",
        "            if (dec_inp == 1).all() or (y is not None and i >= len(y)):\n",
        "                break            \n",
        "            # teacher forcing\n",
        "            elif y is not None and (self.prob_teach_forcing > 0) and (random.random() < self.prob_teach_forcing):\n",
        "                dec_inp = y[i].to(self.device)\n",
        "            else:\n",
        "                dec_inp = dec_output.data.max(1)[1]  # [1] to get argmax\n",
        "        \n",
        "        return torch.stack(res), torch.stack(alphas)\n",
        "    \n",
        "    def encode(self, x):\n",
        "        return self.encoder(x.to(self.device))\n",
        "    \n",
        "    def decode_step(self, dec_inp, h, annotation_vecs):\n",
        "        #pdb.set_trace()\n",
        "        context_vec, alpha = self.att(annotation_vecs, h[-1])\n",
        "        beta = torch.sigmoid(self.f_b(h[-1]))\n",
        "        context_vec = beta * context_vec  # Section 4.2.1 in Xu et al. (2015)\n",
        "        \n",
        "        emb_inp = self.emb(dec_inp).unsqueeze(0)  # adds unit axis at beginning so that rnn 'loops' once\n",
        "\n",
        "        output, h = self.rnn_dec(torch.cat([emb_inp, context_vec.unsqueeze(0)], dim=2), h)\n",
        "        output = self.out(self.out_drop(output[0]))\n",
        "\n",
        "        # return F.log_softmax(output, dim=1), h, alpha\n",
        "        return F.log_softmax(output, dim=1), h, alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCyXa-3TC9lC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CWfULVJ_gDh",
        "colab_type": "code",
        "outputId": "bb8a6b5f-6238-4445-c2f6-f88e34b6e061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "de1ac3a6a7ec4ac5b1bbb6018b301915",
            "7d2c21f8bde346aa8ba27be35d2ad06c",
            "e95113badecd419e8269d082d1d8348b",
            "2193d80a1e6f4d939413efe592e0b22d",
            "39b0cb32e3db4a568e6d574d790ca7bf",
            "83499b3ad71440818edc8b71078b310d",
            "2b16c0bf210347bb82e1bda55854d7f2",
            "5dc0c90163864a18b1b57043778892ab"
          ]
        }
      },
      "source": [
        "decoder = Decoder(device, 7, 2048, len(vocab.itos), 1000, 50, n_layers, p_drop=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de1ac3a6a7ec4ac5b1bbb6018b301915",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=178728960), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a5_DYoPC-RL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c5b5a857-7865-429a-98e9-c87955858a4b",
        "id": "s269W7TDzSRe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ig0HRQnLFhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learn = pickler.load(open('models/learn_cpu.pkl', 'rb')) #use this"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrF-0K6IqJBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = picklizer.load(open('models/loaded_learn_cpu.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdB1V8e9x2x9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from fastai.callbacks import SaveModelCallback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMbfmmstqM7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learn.callback = SaveModelCallback(learn)\n",
        "# learn.loss_func = None\n",
        "# learn.metrics = None\n",
        "# learn.purge()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40YuPAs0NtDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learn.model_dir = Path('models/')\n",
        "# learn.path = Path('.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xpGuLA2nYqll",
        "colab": {}
      },
      "source": [
        "# learn = learn.load('mygoodmodel_coco') #and this to create new_loaded joblib\n",
        "# learn.data = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTzNTSZk01_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# picklizer.dump(learn.model, open('/content/drive/My Drive/Image_Captioning/models/loaded_learn_cpu.pkl', 'wb'), compress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkBX0WEvLP5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXQ7kMmKoIAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decode_eval = learn.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C61zzZItDnSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HypothesisNode():\n",
        "    \"\"\" Hypothesis Node class for performing Beam Search \"\"\"\n",
        "    def __init__(self, sequence, log_prob, hidden_state, alphas):\n",
        "        \"\"\"HypothesisNode constructur\n",
        "        \n",
        "        Args:\n",
        "          sequence: A sequence of tokens\n",
        "          log_prob: The log of the probability of this sequence\n",
        "          hidden_state: The hidden state of the Decoder RNN after decoding the last token in the sequence\n",
        "        \"\"\"\n",
        "        self._seq = sequence\n",
        "        self._alphas = alphas\n",
        "        self._log_prob = log_prob\n",
        "        self._h = hidden_state\n",
        "    \n",
        "    @property\n",
        "    def last_tok(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          The last token in the sequence\n",
        "        \"\"\"\n",
        "        return self._seq[-1]\n",
        "        \n",
        "    def update(self, tok, log_prob, new_h, new_alpha):\n",
        "        \"\"\"\n",
        "        Updates the sequence with a new token and returns a new Hypothesis Node\n",
        "        Args:\n",
        "          tok: The new token that is appended to the sequence\n",
        "          log_prob: The log of the probability ot this token\n",
        "          new_h: The new hidden state of the Decoder RNN after this token\n",
        "        \n",
        "        Returns:\n",
        "          An Hypothesis Node with the updated sequence, log probability and hidden state\n",
        "        \"\"\"\n",
        "        return HypothesisNode(self._seq + [tok], self._log_prob + log_prob, new_h, self._alphas + new_alpha)\n",
        "    \n",
        "    def __str__(self):\n",
        "        return ('Hyp(log_p = %4f,\\t seq = %s)' % (self._log_prob, vocab.textify([t.item()for t in self._seq])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3gtO4OBDswO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BeamSearch():\n",
        "    \"\"\" Performs BeamSearch for seq2seq decoding or Image captioning \"\"\"\n",
        "    def __init__(self, enc_model, dec_model, beam_width=5, num_results=1, max_len=30, device=torch.device('cuda:0')):\n",
        "        \"\"\"BeamSearch object constructor\n",
        "        Args:\n",
        "          enc_model: A seq2seq encoder or cnn for image captioning\n",
        "          dec_model: A RNN decoder model\n",
        "          beam_width: int, the number of hypotheses to remember in each iteration\n",
        "          max_len: int, the longest possible sequence\n",
        "        \"\"\"\n",
        "        self._device = device\n",
        "        self._enc_model = enc_model\n",
        "        self._dec_model = dec_model\n",
        "        self._beam_width = beam_width\n",
        "        self._num_results = num_results\n",
        "        self._max_len = max_len\n",
        "        self._start_tok = 0\n",
        "        self._end_tok   = 1\n",
        "        self._annotation_vecs = None\n",
        "        \n",
        "    def __call__(self, img, verbose=False):\n",
        "        \"\"\"Performs the Beam search\n",
        "        Args:\n",
        "          img: the image to be annotated, torch tensor with 3 color channels\n",
        "          verbose: bool, allows printing the intermediate hypotheses for better understanding\n",
        "        \n",
        "        Returns:\n",
        "          The 'beam_width' most probable sentences\n",
        "        \"\"\"\n",
        "        img = img.unsqueeze(0)\n",
        "        h, annotation_vecs = self._enc_model(img)\n",
        "        self._annotation_vecs = annotation_vecs\n",
        "        \n",
        "        hyps = [HypothesisNode([torch.zeros(1, requires_grad=False).long().to(self._device)], 0, h, [])]\n",
        "        results = []\n",
        "        \n",
        "        step = 0\n",
        "        width = self._beam_width\n",
        "        while width > 0 and step < self._max_len:\n",
        "            if verbose: print(\"\\n Step: \",step)\n",
        "            new_hyps = []\n",
        "            for h in hyps:\n",
        "                new_hyps.extend(self.get_next_hypotheses(h, width))\n",
        "            \n",
        "            new_hyps = sorted(new_hyps, key= lambda x: x._log_prob, reverse=True)\n",
        "            if verbose: self.print_hypotheses(new_hyps, \"Before narrowing:\")\n",
        "                \n",
        "            hyps = []\n",
        "            for h in new_hyps[:width]:\n",
        "                if h.last_tok == self._end_tok:\n",
        "                    results.append(h)\n",
        "                    width = width - 1\n",
        "                else:\n",
        "                    hyps.append(h)\n",
        "            \n",
        "            if verbose: \n",
        "                self.print_hypotheses(hyps, \"After narrowing:\")\n",
        "                self.print_hypotheses(results, \"Results:\")\n",
        "                \n",
        "            step += 1\n",
        "         \n",
        "        results.extend(hyps[:width])\n",
        "        results = sorted(results, key=lambda x: x._log_prob/len(x._seq), reverse=True)\n",
        "        \n",
        "        if verbose: self.print_hypotheses(results, \"Final:\")\n",
        "        \n",
        "        if self._num_results == 1:\n",
        "            return ([t.item() for t in results[0]._seq[1:-1]], torch.stack(results[0]._alphas))\n",
        "        else:\n",
        "            return [([t.item() for t in r._seq[1:-1]], torch.stack(r._alphas)) for r in results[:self._num_results]]\n",
        "        \n",
        "    def get_next_hypotheses(self, hyp, k):\n",
        "        \"\"\"Calculates the next 'beam_width' hypotheses given a Hypothesis Node\n",
        "        Args:\n",
        "          hyp: an Hypothesis Node containing a sequence, a log probability and a Decoder RNN hidden state\n",
        "          k: the number of hypotheses to calculate\n",
        "        Returns:\n",
        "          A list with the 'beam_width' most probable sequences/Hypothesis Nodes\n",
        "        \"\"\"\n",
        "\n",
        "        dec_outp, h, alphas = self._dec_model(hyp.last_tok, hyp._h, self._annotation_vecs)\n",
        "\n",
        "        top_k_log_probs, top_k_toks = dec_outp.topk(k, dim=1)\n",
        "        return [hyp.update(top_k_toks[0][i].unsqueeze(0), top_k_log_probs[0][i], h, list(alphas)) for i in range(k)]\n",
        "    \n",
        "    def print_hypotheses(self, hyps, description):\n",
        "        print(description)\n",
        "        for h in hyps:\n",
        "            print(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0ayjUa1D2b8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj6tC2sv6S3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from scipy.misc import imresize\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from matplotlib.patheffects import Stroke, Normal\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# the functions fig2data and fig2img are taken from \n",
        "# http://www.icare.univ-lille1.fr/tutorials/convert_a_matplotlib_figure\n",
        "# Deprecation errors have been fixed\n",
        "\n",
        "def fig2data ( fig ):\n",
        "    \"\"\"\n",
        "    @brief Convert a Matplotlib figure to a 4D numpy array with RGBA channels and return it\n",
        "    @param fig a matplotlib figure\n",
        "    @return a numpy 3D array of RGBA values\n",
        "    \"\"\"\n",
        "    # draw the renderer\n",
        "    fig.canvas.draw ( )\n",
        " \n",
        "    # Get the RGBA buffer from the figure\n",
        "    w,h = fig.canvas.get_width_height()\n",
        "    buf = np.fromstring( fig.canvas.tostring_argb(), dtype=np.uint8 )\n",
        "    buf.shape = ( w, h,4 )\n",
        " \n",
        "    # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode\n",
        "    buf = np.roll ( buf, 3, axis = 2 )\n",
        "    return buf\n",
        "\n",
        "def fig2img ( fig ):\n",
        "    \"\"\"\n",
        "    @brief Convert a Matplotlib figure to a PIL Image in RGBA format and return it\n",
        "    @param fig a matplotlib figure\n",
        "    @return a Python Imaging Library ( PIL ) image\n",
        "    \"\"\"\n",
        "    # put the figure pixmap into a numpy array\n",
        "    buf = fig2data ( fig )\n",
        "    w, h, d = buf.shape\n",
        "    return Image.frombytes( \"RGBA\", ( w ,h ), buf.tostring( ) )\n",
        "    \n",
        "def draw_text(ax, xy, txt, sz=14):\n",
        "    text = ax.text(*xy, txt, verticalalignment='top', color='white', fontsize=sz, weight='bold')\n",
        "    draw_outline(text, 1)\n",
        "\n",
        "def draw_outline(matplt_plot_obj, lw):\n",
        "    matplt_plot_obj.set_path_effects([Stroke(linewidth=lw, foreground='black'), Normal()])\n",
        "\n",
        "def show_img(im, figsize=None, ax=None, alpha=1, cmap=None):\n",
        "    if not ax:\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.imshow(im, alpha=alpha, cmap=cmap)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    return ax\n",
        "\n",
        "def visualize_attention(im, pred, alphas, denorm, vocab, att_size=7, thresh=0., sz=224, return_fig_as_PIL_image=False):\n",
        "    cap_len = len(pred)\n",
        "    alphas = alphas.view(-1,1,  att_size, att_size).cpu().data.numpy()\n",
        "    alphas = np.maximum(thresh, alphas)\n",
        "    alphas -= alphas.min()\n",
        "    alphas /= alphas.max()\n",
        " \n",
        "    figure, axes = plt.subplots(cap_len//5 + 1,5, figsize=(12,8))\n",
        " \n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i <= cap_len:\n",
        "            ax = show_img(denorm(im), ax=ax)\n",
        "            if i > 0:\n",
        "                mask = np.array(PIL.Image.fromarray(alphas[i - 1,0]).resize((sz,sz)))\n",
        "                blurred_mask = gaussian_filter(mask, sigma=8)\n",
        "                show_img(blurred_mask, ax=ax, alpha=0.5, cmap='afmhot')\n",
        "                draw_text(ax, (0,0), vocab.itos[pred[i - 1]])\n",
        "        else:\n",
        "            ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if return_fig_as_PIL_image:\n",
        "        return fig2img(figure)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biofXRQg1nGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beam_width = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "298W3na74Pb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qaXM8H3X8gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beam_search = BeamSearch(learn.encode, learn.decode_step, beam_width, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPWKuBfXPuIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYGCeM_BY8s6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# url = 'https://st2.depositphotos.com/1761942/9533/i/950/depositphotos_95337166-stock-photo-two-kid-boys-playing-on.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-91GNBdUZTZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mkdir images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCmaC_VGZP_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download_url(url, 'images/rand2.jpg', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAfryLkpJaBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inv_norm = transforms.Normalize(\n",
        "    mean =  [-0.5238/0.3159, -0.5003/0.3091, -0.4718/0.3216],\n",
        "    std = [1/0.3159, 1/0.3091, 1/0.3216]\n",
        ")\n",
        "\n",
        "denorm = transforms.Compose([\n",
        "                            inv_norm,\n",
        "                            # make the image PIL readable\n",
        "                            transforms.functional.to_pil_image\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "231smyO55BZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# im = image2tensor(PILImage.create('images/rand2.jpg').to_thumb(224))/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTRtYFc6HOJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.imshow(im.permute(1,2,0)), im.permute(1,2,0).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo2qZAU3EDOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# results = beam_search(im);\n",
        "# x = vocab.textify(results[0])\n",
        "# print(x)\n",
        "# visualize_attention(im, results[0], results[1], denorm, vocab, att_size=7, sz=sz, thresh=0.02)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6yDE-SsJthy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6NvMYHXOS9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ipywidgets\n",
        "from ipywidgets import widgets, VBox\n",
        "from types import SimpleNamespace\n",
        "import warnings\n",
        "warnings. filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17U3NjvcAF7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "defaults.use_cuda = True\n",
        "text = widgets.Text()\n",
        "output = widgets.Output()\n",
        "output_attn = widgets.Output()\n",
        "label = widgets.Label()\n",
        "caption_lbl = widgets.Label()\n",
        "attend = widgets.Button(description='Caption eet!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWqecePnjGvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyl9doT0CaTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_click(change):\n",
        "    # if len(upload.data)!=0:\n",
        "    if len(upload_btn.data)!=0:\n",
        "        img = PILImage.create(upload_btn.data[-1])\n",
        "        output.clear_output()\n",
        "        with output:\n",
        "            display(img.to_thumb(224))\n",
        "            \n",
        "        gc.collect()\n",
        "        results = beam_search(image2tensor(img.to_thumb(224))/255.);\n",
        "        x = vocab.textify(results[0])\n",
        "        print(x)\n",
        "        pred_caption = vocab.textify(results[0])\n",
        "        caption_lbl.value = pred_caption\n",
        "        output_attn.clear_output()\n",
        "        with output_attn:\n",
        "            visualize_attention(image2tensor(img.to_thumb(224))/255., results[0], results[1], denorm, vocab, att_size=7, sz=sz, thresh=0.02)\n",
        "    else: print('Please upload an image')\n",
        "\n",
        "attend.on_click(on_click)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO0PDjKciFlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# upload = SimpleNamespace(data=[open_image('images/rand2.jpg')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybeDssyJCOX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upload_btn = widgets.FileUpload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PvSUqEdNMQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYJ8bhsvEBQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(VBox([widgets.Label('Attend to something'), \n",
        "      upload_btn, attend, output, caption_lbl]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}